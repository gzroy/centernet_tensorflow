{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "l = tf.keras.layers\n",
    "imageHeight = 512\n",
    "imageWidth = 512\n",
    "\n",
    "sigma = 0.06\n",
    "focal_alpha = 2\n",
    "focal_beta = 4\n",
    "lambda_size = 0.1\n",
    "lambda_offset = 1.0\n",
    "\n",
    "filterDim = [256, 384, 384, 384, 512]\n",
    "\n",
    "resize_width = 550\n",
    "resize_height = 550\n",
    "image_width = 512\n",
    "image_height = 512\n",
    "image_width_delta = resize_width - image_width\n",
    "image_height_delta = resize_height - image_height \n",
    "batch_size = 3\n",
    "valid_batch_size = 1\n",
    "epoch_size = 26332\n",
    "category_num = 80\n",
    "jitter = 0.3\n",
    "vector_size = 1+4+category_num\n",
    "label_vector_size = 1+4+category_num\n",
    "labels = ['umbrella',\n",
    " 'sandwich',\n",
    " 'handbag',\n",
    " 'person',\n",
    " 'snowboard',\n",
    " 'cell phone',\n",
    " 'traffic light',\n",
    " 'potted plant',\n",
    " 'toaster',\n",
    " 'baseball glove',\n",
    " 'cow',\n",
    " 'surfboard',\n",
    " 'remote',\n",
    " 'toilet',\n",
    " 'baseball bat',\n",
    " 'giraffe',\n",
    " 'book',\n",
    " 'bottle',\n",
    " 'stop sign',\n",
    " 'frisbee',\n",
    " 'boat',\n",
    " 'sheep',\n",
    " 'mouse',\n",
    " 'motorcycle',\n",
    " 'car',\n",
    " 'bird',\n",
    " 'pizza',\n",
    " 'bed',\n",
    " 'kite',\n",
    " 'zebra',\n",
    " 'broccoli',\n",
    " 'cat',\n",
    " 'chair',\n",
    " 'bench',\n",
    " 'teddy bear',\n",
    " 'tennis racket',\n",
    " 'laptop',\n",
    " 'sink',\n",
    " 'sports ball',\n",
    " 'skateboard',\n",
    " 'parking meter',\n",
    " 'carrot',\n",
    " 'hair drier',\n",
    " 'banana',\n",
    " 'wine glass',\n",
    " 'scissors',\n",
    " 'spoon',\n",
    " 'cake',\n",
    " 'fire hydrant',\n",
    " 'dog',\n",
    " 'backpack',\n",
    " 'airplane',\n",
    " 'clock',\n",
    " 'keyboard',\n",
    " 'truck',\n",
    " 'bicycle',\n",
    " 'skis',\n",
    " 'bus',\n",
    " 'hot dog',\n",
    " 'dining table',\n",
    " 'cup',\n",
    " 'toothbrush',\n",
    " 'horse',\n",
    " 'elephant',\n",
    " 'refrigerator',\n",
    " 'knife',\n",
    " 'suitcase',\n",
    " 'apple',\n",
    " 'donut',\n",
    " 'couch',\n",
    " 'train',\n",
    " 'microwave',\n",
    " 'bear',\n",
    " 'oven',\n",
    " 'bowl',\n",
    " 'orange',\n",
    " 'tv',\n",
    " 'tie',\n",
    " 'vase',\n",
    " 'fork']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _label_fn(bbox):\n",
    "    stride_height = image_height//4\n",
    "    stride_width = image_width//4\n",
    "    labels = np.zeros((stride_height, stride_width, label_vector_size)) \n",
    "    box_num, _ = bbox.shape\n",
    "    index_x = np.tile(range(stride_width), (stride_height,1))\n",
    "    index_y = np.transpose(index_x)\n",
    "    for i in range(box_num):\n",
    "        center_y = int(bbox[i,0]/4)\n",
    "        center_x = int(bbox[i,1]/4)\n",
    "        box_height = bbox[i,2]/4\n",
    "        box_width = bbox[i,3]/4\n",
    "        box_size = box_height*box_width\n",
    "        category = bbox[i,4]\n",
    "        truth = np.exp(-(np.square(index_x-center_x)+np.square(index_y-center_y))/(2*sigma*box_size))\n",
    "        labels[:,:,category] = np.maximum(truth, labels[:,:,category])\n",
    "        labels[center_y,center_x,-5] = 1.0\n",
    "        labels[center_y,center_x,-4] = float(box_height)\n",
    "        labels[center_y,center_x,-3] = float(box_width)\n",
    "        labels[center_y,center_x,-2] = bbox[i,0]/4 - center_y\n",
    "        labels[center_y,center_x,-1] = bbox[i,1]/4 - center_x\n",
    "    return labels.astype(np.float32)\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    features = {\"image\": tf.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "                \"height\": tf.FixedLenFeature([1], tf.int64, default_value=[0]),\n",
    "                \"width\": tf.FixedLenFeature([1], tf.int64, default_value=[0]),\n",
    "                \"channels\": tf.FixedLenFeature([1], tf.int64, default_value=[3]),\n",
    "                \"colorspace\": tf.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "                \"img_format\": tf.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "                \"label\": tf.VarLenFeature(tf.int64),\n",
    "                \"bbox_xmin\": tf.VarLenFeature(tf.int64),\n",
    "                \"bbox_xmax\": tf.VarLenFeature(tf.int64),\n",
    "                \"bbox_ymin\": tf.VarLenFeature(tf.int64),\n",
    "                \"bbox_ymax\": tf.VarLenFeature(tf.int64),\n",
    "                \"filename\": tf.FixedLenFeature([], tf.string, default_value=\"\")\n",
    "               }\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    \n",
    "    label = tf.expand_dims(parsed_features[\"label\"].values, 0)\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    height = tf.squeeze(tf.cast(parsed_features[\"height\"], tf.int32))\n",
    "    width = tf.squeeze(tf.cast(parsed_features[\"width\"], tf.int32))\n",
    "    channels = parsed_features[\"channels\"]\n",
    "    filename = tf.expand_dims(parsed_features[\"filename\"],0)\n",
    "    #Get the bbox\n",
    "    xmin = tf.cast(tf.expand_dims(parsed_features[\"bbox_xmin\"].values, 0), tf.int32)\n",
    "    xmax = tf.cast(tf.expand_dims(parsed_features[\"bbox_xmax\"].values, 0), tf.int32)\n",
    "    ymin = tf.cast(tf.expand_dims(parsed_features[\"bbox_ymin\"].values, 0), tf.int32)\n",
    "    ymax = tf.cast(tf.expand_dims(parsed_features[\"bbox_ymax\"].values, 0), tf.int32)\n",
    "\n",
    "    boxes = tf.concat([xmin,ymin,xmax,ymax], axis=0)\n",
    "    boxes = tf.transpose(boxes, [1, 0])\n",
    "    #Decode the image\n",
    "    image_raw = tf.image.decode_jpeg(parsed_features[\"image\"], channels=3)\n",
    "    image_decoded = tf.image.convert_image_dtype(image_raw, tf.float32)\n",
    "    \n",
    "    dw = jitter*tf.cast(width, tf.float32)\n",
    "    dh = jitter*tf.cast(height, tf.float32)\n",
    "    new_ar = tf.truediv(tf.add(tf.cast(width, tf.float32), tf.random.uniform([1], minval=tf.math.negative(dw), maxval=dw)), \\\n",
    "                        tf.add(tf.cast(height, tf.float32), tf.random.uniform([1], minval=tf.math.negative(dh), maxval=dh)))\n",
    "    nh, nw = tf.cond(tf.less(new_ar[0],1), \\\n",
    "                     lambda:(image_height, tf.cast(tf.cast(image_height, tf.float32)*new_ar[0], tf.int32)), \\\n",
    "                     lambda:(tf.cast(tf.cast(image_width, tf.float32)/new_ar[0], tf.int32), image_width))\n",
    "    dx = tf.cond(tf.equal(image_width, nw), \\\n",
    "                 lambda:tf.constant([0]), \\\n",
    "                 lambda:tf.random.uniform([1], minval=0, maxval=(image_width-nw), dtype=tf.int32))\n",
    "    dy = tf.cond(tf.equal(image_height, nh), \\\n",
    "                 lambda:tf.constant([0]), \\\n",
    "                 lambda:tf.random.uniform([1], minval=0, maxval=(image_height-nh), dtype=tf.int32))\n",
    "    #image_resize = tf.image.per_image_standardization(tf.image.resize(image_decoded, [nh, nw]))\n",
    "    image_resize = tf.image.resize(image_decoded, [nh, nw])\n",
    "    image_distort = tf.image.per_image_standardization(image_resize)\n",
    "    #image_distort = distort_color(image_resize)\n",
    "    image_padded = tf.image.pad_to_bounding_box(image_distort, dy[0], dx[0], image_height, image_width)\n",
    "\n",
    "    #Adjust the boxes\n",
    "    xmin_new = tf.cast(tf.truediv(nw, width) * tf.cast(xmin,tf.float64), tf.int32) + dx\n",
    "    xmax_new = tf.cast(tf.truediv(nw, width) * tf.cast(xmax,tf.float64), tf.int32) + dx\n",
    "    ymin_new = tf.cast(tf.truediv(nh, height) * tf.cast(ymin,tf.float64), tf.int32) + dy\n",
    "    ymax_new = tf.cast(tf.truediv(nh, height) * tf.cast(ymax,tf.float64), tf.int32) + dy\n",
    "    boxes_width = xmax_new-xmin_new\n",
    "    boxes_height = ymax_new-ymin_new\n",
    "    boxes_area = boxes_width*boxes_height\n",
    "    \n",
    "    # Random flip flag\n",
    "    random_flip_flag = tf.random.uniform([1], minval=0, maxval=1, dtype=tf.float32)\n",
    "    def flip_box():\n",
    "        xmax_flip = image_width - xmin_new\n",
    "        xmin_flip = image_width - xmax_new\n",
    "        image_flip = tf.image.flip_left_right(image_padded)\n",
    "        return xmin_flip, xmax_flip, image_flip\n",
    "    def notflip():\n",
    "        return xmin_new, xmax_new, image_padded\n",
    "    xmin_flip, xmax_flip, image_flip = tf.cond(tf.less(random_flip_flag[0], 0.5), notflip, flip_box)\n",
    "    center_x = xmin_flip + (xmax_flip-xmin_flip)//2\n",
    "    center_y = ymin_new + (ymax_new-ymin_new)//2\n",
    "    boxes_new = tf.concat([center_y,center_x,(ymax_new-ymin_new),(xmax_flip-xmin_flip),label], axis=0)\n",
    "    boxes_new = tf.transpose(boxes_new, [1, 0])\n",
    "    \n",
    "    label_new = tf.py_func(_label_fn, [boxes_new], tf.float32)\n",
    "    \n",
    "    return image_flip, label_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    train_files = tf.data.Dataset.list_files(\"coco/train_10.tfrecord\")\n",
    "    dataset_train = train_files.interleave(tf.data.TFRecordDataset, cycle_length=4, num_parallel_calls=4)\n",
    "    dataset_train = dataset_train.shuffle(buffer_size=epoch_size)\n",
    "    dataset_train = dataset_train.repeat(100)\n",
    "    dataset_train = dataset_train.map(_parse_function, num_parallel_calls=12)\n",
    "    #dataset_train = dataset_train.padded_batch(batch_size, \\\n",
    "    #                    padded_shapes=([None,None, None], [None, None], [None, None, None]))\n",
    "    dataset_train = dataset_train.batch(batch_size)\n",
    "    dataset_train = dataset_train.prefetch(batch_size)\n",
    "    return dataset_train\n",
    "    '''\n",
    "    iterator = tf.data.Iterator.from_structure(dataset_train.output_types, dataset_train.output_shapes)\n",
    "    image_augment, bbox_result, label_result = iterator.get_next()\n",
    "    train_init_op = iterator.make_initializer(dataset_train)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = label_result[:,:,:,:80]\n",
    "labels_obj_mask = tf.equal(label_test,1.0)\n",
    "labels_noobj_mask = tf.less(label_test,1.0)\n",
    "labels_obj = tf.boolean_mask(label_test, labels_obj_mask)\n",
    "labels_noobj = tf.boolean_mask(label_test, labels_noobj_mask)\n",
    "#preds_obj_k = tf.boolean_mask(labels, labels_obj_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(train_init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, bbox, label, label_1, label_2, mask1 = sess.run([image_augment, bbox_result, label_result, labels_obj, labels_noobj, labels_obj_mask])\n",
    "#image, label = sess.run([image_augment, label_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bbox = bbox[0]\n",
    "for i in range(image_bbox.shape[0]):\n",
    "    cv2.rectangle(image[0], (image_bbox[i][0],image_bbox[i][1]), (image_bbox[i][2],image_bbox[i][3]), (0,255,0), 2)\n",
    "plt.imshow(image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encpasulate the conv and residual functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv(inputs, filters, kernel_size, strides, padding, bias=False, normalize=True, activation='relu'):\n",
    "    output = inputs\n",
    "    padding_str = 'same'\n",
    "    if padding>0:\n",
    "        output = l.ZeroPadding2D(padding=padding)(output)\n",
    "        padding_str = 'valid'\n",
    "    output = l.Conv2D(filters, kernel_size, strides, padding_str, use_bias=bias, \\\n",
    "                 kernel_initializer='he_normal', \\\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(l=5e-4))(output)\n",
    "    if normalize:\n",
    "        output = l.BatchNormalization(axis=3)(output)\n",
    "    if activation=='relu':\n",
    "        output = l.ReLU()(output)\n",
    "    if activation=='relu6':\n",
    "        output = l.ReLU(max_value=6)(output)\n",
    "    if activation=='leaky_relu':\n",
    "        output = l.LeakyReLU(alpha=0.1)(output)\n",
    "    if activation=='sigmoid':\n",
    "        output = tf.keras.activations.sigmoid(output)\n",
    "    return output\n",
    "\n",
    "def _residual(inputs, filters, strides):\n",
    "    shortcut = inputs\n",
    "    num_channels = shortcut.get_shape().as_list()[-1]\n",
    "    output = _conv(inputs, filters, 3, strides, 1)\n",
    "    output = _conv(output, filters, 3, 1, 0, False, True, 'linear')\n",
    "    if num_channels != filters or strides != 1:\n",
    "        shortcut = _conv(shortcut, filters, 1, strides, 0, False, True, 'linear')\n",
    "    output = l.Add()([output, shortcut])\n",
    "    output = l.ReLU()(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hourglass function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hourglass(inputs, filterDim):\n",
    "    #Left part, left_features image dimension, 1,1/2,1/4,1/8,1/16,1/32\n",
    "    left_features = [inputs]\n",
    "    for index, dim in enumerate(filterDim):\n",
    "        output = _residual(left_features[-1], dim, 2)\n",
    "        output = _residual(output, dim, 1)\n",
    "        left_features.append(output)\n",
    "    #Middle part\n",
    "    output = left_features[-1]\n",
    "    for i in range(5):\n",
    "        output = _residual(output, filterDim[-1], 1)\n",
    "    #Right part\n",
    "    for index in reversed(range(len(filterDim))):\n",
    "        output = _residual(output, filterDim[index], 1)\n",
    "        output = _residual(output, filterDim[max(index-1, 0)], 1)\n",
    "        output = l.UpSampling2D()(output)\n",
    "        left_feature = _residual(left_features[index], filterDim[max(index-1, 0)], 1)\n",
    "        left_feature = _residual(left_feature, filterDim[max(index-1, 0)], 1)\n",
    "        output = l.Add()([output, left_feature])\n",
    "    output = _conv(output, 256, 3, 1, 1)\n",
    "    outputs = [output]\n",
    "    #Prediction\n",
    "    pred_keypoints = _conv(output, 256, 3, 1, 0, True, False, 'relu') \n",
    "    pred_keypoints = _conv(pred_keypoints, 80, 1, 1, 0, True, False, 'sigmoid') \n",
    "    outputs.append(pred_keypoints)\n",
    "    pred_offset = _conv(output, 256, 3, 1, 0, True, False, 'relu') \n",
    "    pred_offset = _conv(pred_offset, 2, 1, 1, 0, True, False, 'linear') \n",
    "    outputs.append(pred_offset)\n",
    "    pred_size = _conv(output, 256, 3, 1, 0, True, False, 'relu') \n",
    "    pred_size = _conv(pred_size, 2, 1, 1, 0, True, False, 'linear') \n",
    "    outputs.append(pred_size)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the base hourglass network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassNetwork():\n",
    "    image = tf.keras.Input(shape=(imageHeight,imageWidth,3))   #512×512×3\n",
    "    net = _conv(image, 128, 7, 2, 3)    #256×256×128\n",
    "    net = _residual(net, 256, 2)        #128×128×256\n",
    "    outputs_1 = _hourglass(net, filterDim)\n",
    "    outputs_2 = _hourglass(outputs_1[0], filterDim)\n",
    "    model = tf.keras.Model(inputs=image, outputs=(outputs_1+outputs_2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds, labels):\n",
    "    labels_offset_size = labels[...,80:]\n",
    "    labels_offset_size_mask = tf.equal(labels_offset_size[...,0],1.0)\n",
    "    labels_offset_size_obj = tf.boolean_mask(labels_offset_size, labels_offset_size_mask)\n",
    "    obj_num = tf.cast(labels_offset_size_obj.shape[0], tf.float32)\n",
    "    preds_offset = preds[1]\n",
    "    preds_offset_obj = tf.boolean_mask(preds_offset, labels_offset_size_mask)\n",
    "    loss_offset = tf.math.truediv(\\\n",
    "                      tf.reduce_sum(\\\n",
    "                          tf.math.abs(\\\n",
    "                              preds_offset_obj-labels_offset_size_obj[...,1:3])),\n",
    "                      obj_num)\n",
    "    preds_size = preds[2]\n",
    "    preds_size_obj = tf.boolean_mask(preds_size, labels_offset_size_mask)\n",
    "    loss_size = tf.math.truediv(\\\n",
    "                      tf.reduce_sum(\\\n",
    "                          tf.math.abs(\\\n",
    "                              preds_size_obj-labels_offset_size_obj[...,3:])),\n",
    "                      obj_num)\n",
    "    labels_k = labels[...,:80]\n",
    "    labels_k_obj_mask = tf.equal(labels_k, 1.0)\n",
    "    labels_k_noobj_mask = tf.less(labels_k, 1.0)\n",
    "    labels_k_obj = tf.boolean_mask(labels_k, labels_k_obj_mask)\n",
    "    labels_k_noobj = tf.boolean_mask(labels_k, labels_k_noobj_mask)\n",
    "    preds_k_obj = tf.boolean_mask(preds[0], labels_k_obj_mask)\n",
    "    preds_k_noobj = tf.boolean_mask(preds[0], labels_k_noobj_mask)\n",
    "    loss_k_obj = tf.reduce_sum(\\\n",
    "                     tf.math.multiply(\\\n",
    "                         tf.math.pow((1.0-preds_k_obj), focal_alpha),\\\n",
    "                         tf.math.log(preds_k_obj)))\n",
    "    loss_k_noobj = tf.reduce_sum(\\\n",
    "                       tf.math.multiply(\\\n",
    "                           tf.math.multiply(\\\n",
    "                               tf.math.pow((1.0-labels_k_noobj), focal_beta),\n",
    "                               tf.math.pow(preds_k_noobj, focal_alpha)),\n",
    "                           tf.math.log(1.0-preds_k_noobj)))\n",
    "    loss_k = tf.truediv(\\\n",
    "                 (loss_k_obj+loss_k_noobj),\\\n",
    "                 obj_num) \n",
    "    loss = loss_k + lambda_size*loss_size + lambda_offset*loss_offset\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate the output to prediction bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(keypoint, offset, size):\n",
    "    keypoint_nms = l.MaxPool2D(3, 1, 'same')(keypoint)\n",
    "    keypoint_mask = tf.cast(tf.equal(keypoint, keypoint_nms), tf.float32)\n",
    "    keypoint = keypoint * keypoint_mask\n",
    "    keypoint_shape = tf.shape(keypoint)\n",
    "    batch = keypoint_shape[0]\n",
    "    height = keypoint_shape[1]\n",
    "    width = keypoint_shape[2]\n",
    "    category = keypoint_shape[3]\n",
    "    keypoint_flat = tf.reshape(keypoint, [batch, -1])\n",
    "    offset_flat = tf.reshape(offset, [batch, -1, 2])\n",
    "    size_flat = tf.reshape(size, [batch, -1, 2])\n",
    "    scores, indices = tf.math.top_k(keypoint_flat, k=10, sorted=True)\n",
    "    classes = tf.cast(indices%category, tf.float32)\n",
    "    indices = tf.cast(indices/category, tf.int32)\n",
    "    x = tf.cast(indices%width, tf.float32)\n",
    "    y = tf.cast(indices/height, tf.float32)\n",
    "    offset_x = tf.batch_gather(offset_flat[...,1], indices)\n",
    "    offset_y = tf.batch_gather(offset_flat[...,0], indices)\n",
    "    size_w = tf.batch_gather(size_flat[...,1], indices)\n",
    "    size_h = tf.batch_gather(size_flat[...,0], indices)\n",
    "    x = x + offset_x\n",
    "    y = y + offset_y\n",
    "    xmin = (x - size_w/2)*4\n",
    "    xmax = (x + size_w/2)*4\n",
    "    ymin = (y - size_h/2)*4\n",
    "    ymax = (y + size_h/2)*4\n",
    "    bbox = tf.stack([xmin,xmax,ymin,ymax,scores,classes], axis=-1)\n",
    "    return bbox\n",
    "\n",
    "keypoint = tf.random.uniform([3,8,8,4])\n",
    "offset = tf.random.uniform([3,8,8,2])\n",
    "size = tf.random.uniform([3,8,8,2],minval=0,maxval=100)\n",
    "bbox = predict(keypoint,offset,size)\n",
    "bbox_mask = tf.greater(bbox[...,4],0.99)\n",
    "bbox_nms = tf.boolean_mask(bbox, bbox_mask)\n",
    "\n",
    "sess=tf.Session()\n",
    "k,o,s,b,b_nms = sess.run([keypoint,offset,size, bbox, bbox_nms])\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_nms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(keypoint, offset, size):\n",
    "    keypoint_nms = l.MaxPool2D(3, 1, 'same')(keypoint)\n",
    "    keypoint_mask = tf.cast(tf.equal(keypoint, keypoint_nms), tf.float32)\n",
    "    keypoint = keypoint * keypoint_mask\n",
    "    keypoint_shape = tf.shape(keypoint)\n",
    "    batch = keypoint_shape[0]\n",
    "    height = keypoint_shape[1]\n",
    "    width = keypoint_shape[2]\n",
    "    category = keypoint_shape[3]\n",
    "    keypoint_flat = tf.reshape(keypoint, [batch, -1])\n",
    "    offset_flat = tf.reshape(offset, [batch, -1, 2])\n",
    "    size_flat = tf.reshape(size, [batch, -1, 2])\n",
    "    scores, indices = tf.math.top_k(keypoint_flat, k=10, sorted=True)\n",
    "    classes = tf.cast(indices%category, tf.float32)\n",
    "    indices = tf.cast(indices/category, tf.int32)\n",
    "    x = tf.cast(indices%width, tf.float32)\n",
    "    y = tf.cast(indices/height, tf.float32)\n",
    "    offset_x = tf.batch_gather(offset_flat[...,1], indices)\n",
    "    offset_y = tf.batch_gather(offset_flat[...,0], indices)\n",
    "    size_w = tf.batch_gather(size_flat[...,1], indices)\n",
    "    size_h = tf.batch_gather(size_flat[...,0], indices)\n",
    "    x = x + offset_x\n",
    "    y = y + offset_y\n",
    "    xmin = (x - size_w/2)*4\n",
    "    xmax = (x + size_w/2)*4\n",
    "    ymin = (y - size_h/2)*4\n",
    "    ymax = (y + size_h/2)*4\n",
    "    bbox = tf.stack([xmin,xmax,ymin,ymax,scores,classes], axis=-1)\n",
    "    return bbox\n",
    "                                \n",
    "def CenterModel(features, labels, mode, params):\n",
    "    model = HourglassNetwork()\n",
    "    training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    images = tf.reshape(features[\"images\"], [-1, imageHeight, imageWidth, 3])\n",
    "    outputs = model(images, training)\n",
    " \n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"bboxes\": prdict(outputs[5], outputs[6], outputs[7])\n",
    "    }\n",
    " \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions, \\\n",
    "                                          export_outputs={'classify': tf.estimator.export.PredictOutput(predictions)})\n",
    " \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss_inter = loss_fn(outputs[1:4], labels)\n",
    "    loss_final = loss_fn(outputs[5:], labels)\n",
    "    loss = loss_inter + loss_final\n",
    " \n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        '''\n",
    "        global_step = tf.train.get_global_step()\n",
    "        boundaries = [5000, 60000, 80000]\n",
    "        values = [0.1, 0.01, 0.001, 0.0001]\n",
    "        learning_rate = tf.compat.v1.train.piecewise_constant(global_step, boundaries, values)\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "        '''\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(model.get_updates_for(features)):\n",
    "            train_op = optimizer.minimize(loss=loss, global_step=global_step)\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    ''' \n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    m = tf.keras.metrics.sparse_top_k_categorical_accuracy(y_true=labels,  y_pred=logits)\n",
    "    tf.summary.scalar('top-5_accuracy', m)\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "    tf.summary.scalar('accuracy', accuracy[0]) \n",
    "    eval_metric_ops = {\n",
    "        #\"accuracy\": tf.metrics.accuracy(labels=true_labels, predictions=predictions[\"classes\"])}\n",
    "        \"accuracy\": accuracy} \n",
    "        #\"top-5 accuracy\": (m.result(), m.update_state(y_true=labels, y_pred=logits))}\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 08:31:14.347097 140447311755008 estimator.py:1790] Using default config.\n",
      "I0924 08:31:14.349068 140447311755008 estimator.py:209] Using config: {'_model_dir': '/home/roy/AI/centermodel/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbbf83d90f0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "W0924 08:31:14.370919 140447311755008 deprecation.py:323] From /home/roy/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0924 08:31:14.668092 140447311755008 deprecation.py:323] From /home/roy/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0924 08:31:14.816411 140447311755008 deprecation.py:323] From <ipython-input-2-debc92f1c2a7>:101: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "I0924 08:31:14.880756 140447311755008 estimator.py:1145] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"batch_normalization_2/cond/Merge:0\", shape=(?, 128, 128, 256), dtype=float32)\n",
      "2\n",
      "Tensor(\"batch_normalization_3/cond/Merge:0\", shape=(?, 128, 128, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_5/cond/Merge:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "2\n",
      "Tensor(\"batch_normalization_6/cond/Merge:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_8/cond/Merge:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_4/Relu:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_10/cond/Merge:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "2\n",
      "Tensor(\"batch_normalization_11/cond/Merge:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_13/cond/Merge:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_8/Relu:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_15/cond/Merge:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "2\n",
      "Tensor(\"batch_normalization_16/cond/Merge:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_18/cond/Merge:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_12/Relu:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_20/cond/Merge:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "2\n",
      "Tensor(\"batch_normalization_21/cond/Merge:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_23/cond/Merge:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_16/Relu:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_25/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "2\n",
      "Tensor(\"batch_normalization_26/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_28/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_20/Relu:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_30/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_22/Relu:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_32/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_24/Relu:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_34/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_26/Relu:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_36/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_28/Relu:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_38/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_30/Relu:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_40/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_32/Relu:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_42/cond/Merge:0\", shape=(?, 4, 4, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"batch_normalization_43/cond/Merge:0\", shape=(?, 4, 4, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_45/cond/Merge:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_18/Relu:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_47/cond/Merge:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_38/Relu:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_49/cond/Merge:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"add_20/add:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_51/cond/Merge:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_42/Relu:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_53/cond/Merge:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_14/Relu:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_55/cond/Merge:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_46/Relu:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_57/cond/Merge:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"add_25/add:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_59/cond/Merge:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_50/Relu:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_61/cond/Merge:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_10/Relu:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_63/cond/Merge:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_54/Relu:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_65/cond/Merge:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"add_30/add:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_67/cond/Merge:0\", shape=(?, 32, 32, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"batch_normalization_68/cond/Merge:0\", shape=(?, 32, 32, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_70/cond/Merge:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_6/Relu:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_72/cond/Merge:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_62/Relu:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_74/cond/Merge:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"add_35/add:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_76/cond/Merge:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_66/Relu:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_78/cond/Merge:0\", shape=(?, 128, 128, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_2/Relu:0\", shape=(?, 128, 128, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_80/cond/Merge:0\", shape=(?, 128, 128, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_70/Relu:0\", shape=(?, 128, 128, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_83/cond/Merge:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "2\n",
      "Tensor(\"batch_normalization_84/cond/Merge:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_86/cond/Merge:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_78/Relu:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_88/cond/Merge:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "2\n",
      "Tensor(\"batch_normalization_89/cond/Merge:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_91/cond/Merge:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_82/Relu:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_93/cond/Merge:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "2\n",
      "Tensor(\"batch_normalization_94/cond/Merge:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_96/cond/Merge:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_86/Relu:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_98/cond/Merge:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "2\n",
      "Tensor(\"batch_normalization_99/cond/Merge:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_101/cond/Merge:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_90/Relu:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_103/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "2\n",
      "Tensor(\"batch_normalization_104/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_106/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_94/Relu:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_108/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_96/Relu:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_110/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_98/Relu:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_112/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_100/Relu:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_114/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_102/Relu:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_116/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_104/Relu:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_118/cond/Merge:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_106/Relu:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_120/cond/Merge:0\", shape=(?, 4, 4, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"batch_normalization_121/cond/Merge:0\", shape=(?, 4, 4, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_123/cond/Merge:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_92/Relu:0\", shape=(?, 8, 8, 384), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"batch_normalization_125/cond/Merge:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_112/Relu:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_127/cond/Merge:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"add_60/add:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_129/cond/Merge:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_116/Relu:0\", shape=(?, 8, 8, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_131/cond/Merge:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_88/Relu:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_133/cond/Merge:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_120/Relu:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_135/cond/Merge:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"add_65/add:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_137/cond/Merge:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_124/Relu:0\", shape=(?, 16, 16, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_139/cond/Merge:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_84/Relu:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_141/cond/Merge:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_128/Relu:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_143/cond/Merge:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "1\n",
      "Tensor(\"add_70/add:0\", shape=(?, 32, 32, 384), dtype=float32)\n",
      "Tensor(\"batch_normalization_145/cond/Merge:0\", shape=(?, 32, 32, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"batch_normalization_146/cond/Merge:0\", shape=(?, 32, 32, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_148/cond/Merge:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_80/Relu:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_150/cond/Merge:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_136/Relu:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_152/cond/Merge:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"add_75/add:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_154/cond/Merge:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_140/Relu:0\", shape=(?, 64, 64, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_156/cond/Merge:0\", shape=(?, 128, 128, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_73/Relu:0\", shape=(?, 128, 128, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_158/cond/Merge:0\", shape=(?, 128, 128, 256), dtype=float32)\n",
      "1\n",
      "Tensor(\"re_lu_144/Relu:0\", shape=(?, 128, 128, 256), dtype=float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4f603e88af61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4f603e88af61>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                  params={'feature_columns': my_feature_columns,})\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mimagenet_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#eval_results = imagenet_classifier.evaluate(input_fn=val_input_fn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(eval_results)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1186\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1188\u001b[0;31m           features, labels, ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m   1189\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-01f8faba2ff8>\u001b[0m in \u001b[0;36mCenterModel\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimageHeight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimageWidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    642\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m       \u001b[0m_check_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_check_index\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;31m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;31m# will break `_slice_helper` contract.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SLICE_TYPE_ERROR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", got {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'images'"
     ]
    }
   ],
   "source": [
    "def main(_):\n",
    "    my_feature_columns = []\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key='images', shape=(imageHeight,imageWidth, 3)))\n",
    "    imagenet_classifier = tf.estimator.Estimator(model_fn=CenterModel, \\\n",
    "                                                 model_dir=\"/home/roy/AI/centermodel/\", \\\n",
    "                                                 params={'feature_columns': my_feature_columns,})\n",
    "    for _ in range(10):\n",
    "        imagenet_classifier.train(input_fn=train_input_fn, steps=5000)\n",
    "        #eval_results = imagenet_classifier.evaluate(input_fn=val_input_fn)\n",
    "        #print(eval_results)\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
