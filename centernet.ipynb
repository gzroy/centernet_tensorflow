{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roy/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/roy/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/roy/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/roy/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/roy/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/roy/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/roy/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/roy/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/roy/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/roy/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/roy/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/roy/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "l = tf.keras.layers\n",
    "imageHeight = 512\n",
    "imageWidth = 512\n",
    "\n",
    "sigma = 0.06\n",
    "focal_alpha = 2\n",
    "focal_beta = 4\n",
    "lambda_size = 0.1\n",
    "lambda_offset = 1.0\n",
    "\n",
    "filterDim = [256, 384, 384, 384, 512]\n",
    "\n",
    "train_files_names = os.listdir('/home/roy/data/coco/train_tf/')\n",
    "train_files = ['/home/roy/data/coco/train_tf/'+item for item in train_files_names]\n",
    "\n",
    "resize_width = 550\n",
    "resize_height = 550\n",
    "image_width = 512\n",
    "image_height = 512\n",
    "image_width_delta = resize_width - image_width\n",
    "image_height_delta = resize_height - image_height \n",
    "batch_size = 64\n",
    "valid_batch_size = 1\n",
    "epoch_size = 26332\n",
    "category_num = 80\n",
    "jitter = 0.3\n",
    "vector_size = 1+4+category_num\n",
    "label_vector_size = 1+4+category_num\n",
    "labels = ['umbrella',\n",
    " 'sandwich',\n",
    " 'handbag',\n",
    " 'person',\n",
    " 'snowboard',\n",
    " 'cell phone',\n",
    " 'traffic light',\n",
    " 'potted plant',\n",
    " 'toaster',\n",
    " 'baseball glove',\n",
    " 'cow',\n",
    " 'surfboard',\n",
    " 'remote',\n",
    " 'toilet',\n",
    " 'baseball bat',\n",
    " 'giraffe',\n",
    " 'book',\n",
    " 'bottle',\n",
    " 'stop sign',\n",
    " 'frisbee',\n",
    " 'boat',\n",
    " 'sheep',\n",
    " 'mouse',\n",
    " 'motorcycle',\n",
    " 'car',\n",
    " 'bird',\n",
    " 'pizza',\n",
    " 'bed',\n",
    " 'kite',\n",
    " 'zebra',\n",
    " 'broccoli',\n",
    " 'cat',\n",
    " 'chair',\n",
    " 'bench',\n",
    " 'teddy bear',\n",
    " 'tennis racket',\n",
    " 'laptop',\n",
    " 'sink',\n",
    " 'sports ball',\n",
    " 'skateboard',\n",
    " 'parking meter',\n",
    " 'carrot',\n",
    " 'hair drier',\n",
    " 'banana',\n",
    " 'wine glass',\n",
    " 'scissors',\n",
    " 'spoon',\n",
    " 'cake',\n",
    " 'fire hydrant',\n",
    " 'dog',\n",
    " 'backpack',\n",
    " 'airplane',\n",
    " 'clock',\n",
    " 'keyboard',\n",
    " 'truck',\n",
    " 'bicycle',\n",
    " 'skis',\n",
    " 'bus',\n",
    " 'hot dog',\n",
    " 'dining table',\n",
    " 'cup',\n",
    " 'toothbrush',\n",
    " 'horse',\n",
    " 'elephant',\n",
    " 'refrigerator',\n",
    " 'knife',\n",
    " 'suitcase',\n",
    " 'apple',\n",
    " 'donut',\n",
    " 'couch',\n",
    " 'train',\n",
    " 'microwave',\n",
    " 'bear',\n",
    " 'oven',\n",
    " 'bowl',\n",
    " 'orange',\n",
    " 'tv',\n",
    " 'tie',\n",
    " 'vase',\n",
    " 'fork']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _label_fn(bbox):\n",
    "    stride_height = image_height//4\n",
    "    stride_width = image_width//4\n",
    "    labels = np.zeros((stride_height, stride_width, label_vector_size)) \n",
    "    box_num, _ = bbox.shape\n",
    "    index_x = np.tile(range(stride_width), (stride_height,1))\n",
    "    index_y = np.transpose(index_x)\n",
    "    for i in range(box_num):\n",
    "        center_y = int(bbox[i,0]/4)\n",
    "        center_x = int(bbox[i,1]/4)\n",
    "        box_height = bbox[i,2]/4\n",
    "        box_width = bbox[i,3]/4\n",
    "        box_size = box_height*box_width\n",
    "        category = bbox[i,4]\n",
    "        truth = np.exp(-(np.square(index_x-center_x)+np.square(index_y-center_y))/(2*sigma*box_size))\n",
    "        labels[:,:,category] = np.maximum(truth, labels[:,:,category])\n",
    "        labels[center_y,center_x,-5] = 1.0\n",
    "        labels[center_y,center_x,-4] = float(box_height)\n",
    "        labels[center_y,center_x,-3] = float(box_width)\n",
    "        labels[center_y,center_x,-2] = bbox[i,0]/4 - center_y\n",
    "        labels[center_y,center_x,-1] = bbox[i,1]/4 - center_x\n",
    "    return labels.astype(np.float32)\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    features = {\"image\": tf.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "                \"height\": tf.FixedLenFeature([1], tf.int64, default_value=[0]),\n",
    "                \"width\": tf.FixedLenFeature([1], tf.int64, default_value=[0]),\n",
    "                \"channels\": tf.FixedLenFeature([1], tf.int64, default_value=[3]),\n",
    "                \"colorspace\": tf.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "                \"img_format\": tf.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "                \"label\": tf.VarLenFeature(tf.int64),\n",
    "                \"bbox_xmin\": tf.VarLenFeature(tf.int64),\n",
    "                \"bbox_xmax\": tf.VarLenFeature(tf.int64),\n",
    "                \"bbox_ymin\": tf.VarLenFeature(tf.int64),\n",
    "                \"bbox_ymax\": tf.VarLenFeature(tf.int64),\n",
    "                \"filename\": tf.FixedLenFeature([], tf.string, default_value=\"\")\n",
    "               }\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    \n",
    "    label = tf.expand_dims(parsed_features[\"label\"].values, 0)\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    height = tf.squeeze(tf.cast(parsed_features[\"height\"], tf.int32))\n",
    "    width = tf.squeeze(tf.cast(parsed_features[\"width\"], tf.int32))\n",
    "    channels = parsed_features[\"channels\"]\n",
    "    filename = tf.expand_dims(parsed_features[\"filename\"],0)\n",
    "    #Get the bbox\n",
    "    xmin = tf.cast(tf.expand_dims(parsed_features[\"bbox_xmin\"].values, 0), tf.int32)\n",
    "    xmax = tf.cast(tf.expand_dims(parsed_features[\"bbox_xmax\"].values, 0), tf.int32)\n",
    "    ymin = tf.cast(tf.expand_dims(parsed_features[\"bbox_ymin\"].values, 0), tf.int32)\n",
    "    ymax = tf.cast(tf.expand_dims(parsed_features[\"bbox_ymax\"].values, 0), tf.int32)\n",
    "\n",
    "    boxes = tf.concat([xmin,ymin,xmax,ymax], axis=0)\n",
    "    boxes = tf.transpose(boxes, [1, 0])\n",
    "    #Decode the image\n",
    "    image_raw = tf.image.decode_jpeg(parsed_features[\"image\"], channels=3)\n",
    "    image_decoded = tf.image.convert_image_dtype(image_raw, tf.float32)\n",
    "    \n",
    "    dw = jitter*tf.cast(width, tf.float32)\n",
    "    dh = jitter*tf.cast(height, tf.float32)\n",
    "    new_ar = tf.truediv(tf.add(tf.cast(width, tf.float32), tf.random.uniform([1], minval=tf.math.negative(dw), maxval=dw)), \\\n",
    "                        tf.add(tf.cast(height, tf.float32), tf.random.uniform([1], minval=tf.math.negative(dh), maxval=dh)))\n",
    "    nh, nw = tf.cond(tf.less(new_ar[0],1), \\\n",
    "                     lambda:(image_height, tf.cast(tf.cast(image_height, tf.float32)*new_ar[0], tf.int32)), \\\n",
    "                     lambda:(tf.cast(tf.cast(image_width, tf.float32)/new_ar[0], tf.int32), image_width))\n",
    "    dx = tf.cond(tf.equal(image_width, nw), \\\n",
    "                 lambda:tf.constant([0]), \\\n",
    "                 lambda:tf.random.uniform([1], minval=0, maxval=(image_width-nw), dtype=tf.int32))\n",
    "    dy = tf.cond(tf.equal(image_height, nh), \\\n",
    "                 lambda:tf.constant([0]), \\\n",
    "                 lambda:tf.random.uniform([1], minval=0, maxval=(image_height-nh), dtype=tf.int32))\n",
    "    #image_resize = tf.image.per_image_standardization(tf.image.resize(image_decoded, [nh, nw]))\n",
    "    image_resize = tf.image.resize(image_decoded, [nh, nw])\n",
    "    image_distort = tf.image.per_image_standardization(image_resize)\n",
    "    #image_distort = distort_color(image_resize)\n",
    "    image_padded = tf.image.pad_to_bounding_box(image_distort, dy[0], dx[0], image_height, image_width)\n",
    "\n",
    "    #Adjust the boxes\n",
    "    xmin_new = tf.cast(tf.truediv(nw, width) * tf.cast(xmin,tf.float64), tf.int32) + dx\n",
    "    xmax_new = tf.cast(tf.truediv(nw, width) * tf.cast(xmax,tf.float64), tf.int32) + dx\n",
    "    ymin_new = tf.cast(tf.truediv(nh, height) * tf.cast(ymin,tf.float64), tf.int32) + dy\n",
    "    ymax_new = tf.cast(tf.truediv(nh, height) * tf.cast(ymax,tf.float64), tf.int32) + dy\n",
    "    boxes_width = xmax_new-xmin_new\n",
    "    boxes_height = ymax_new-ymin_new\n",
    "    boxes_area = boxes_width*boxes_height\n",
    "    \n",
    "    # Random flip flag\n",
    "    random_flip_flag = tf.random.uniform([1], minval=0, maxval=1, dtype=tf.float32)\n",
    "    def flip_box():\n",
    "        xmax_flip = image_width - xmin_new\n",
    "        xmin_flip = image_width - xmax_new\n",
    "        image_flip = tf.image.flip_left_right(image_padded)\n",
    "        return xmin_flip, xmax_flip, image_flip\n",
    "    def notflip():\n",
    "        return xmin_new, xmax_new, image_padded\n",
    "    xmin_flip, xmax_flip, image_flip = tf.cond(tf.less(random_flip_flag[0], 0.5), notflip, flip_box)\n",
    "    center_x = xmin_flip + (xmax_flip-xmin_flip)//2\n",
    "    center_y = ymin_new + (ymax_new-ymin_new)//2\n",
    "    boxes_new = tf.concat([center_y,center_x,(ymax_new-ymin_new),(xmax_flip-xmin_flip),label], axis=0)\n",
    "    boxes_new = tf.transpose(boxes_new, [1, 0])\n",
    "    \n",
    "    label_new = tf.py_func(_label_fn, [boxes_new], tf.float32)\n",
    "    features = {'images': image_flip}\n",
    "    return features, label_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    '''\n",
    "    train_files = tf.data.Dataset.list_files(\"/home/roy/data/coco/train/train_10.tfrecord\")\n",
    "    dataset_train = train_files.interleave(tf.data.TFRecordDataset, cycle_length=4, num_parallel_calls=4)\n",
    "    '''\n",
    "    dataset_train = tf.data.TFRecordDataset(train_files)\n",
    "    dataset_train = dataset_train.shuffle(buffer_size=epoch_size)\n",
    "    dataset_train = dataset_train.repeat(100)\n",
    "    dataset_train = dataset_train.map(_parse_function, num_parallel_calls=12)\n",
    "    #dataset_train = dataset_train.padded_batch(batch_size, \\\n",
    "    #                    padded_shapes=([None,None, None], [None, None], [None, None, None]))\n",
    "    dataset_train = dataset_train.batch(batch_size)\n",
    "    dataset_train = dataset_train.prefetch(batch_size)\n",
    "    return dataset_train\n",
    "    '''\n",
    "    iterator = tf.data.Iterator.from_structure(dataset_train.output_types, dataset_train.output_shapes)\n",
    "    image_augment, bbox_result, label_result = iterator.get_next()\n",
    "    train_init_op = iterator.make_initializer(dataset_train)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = label_result[:,:,:,:80]\n",
    "labels_obj_mask = tf.equal(label_test,1.0)\n",
    "labels_noobj_mask = tf.less(label_test,1.0)\n",
    "labels_obj = tf.boolean_mask(label_test, labels_obj_mask)\n",
    "labels_noobj = tf.boolean_mask(label_test, labels_noobj_mask)\n",
    "#preds_obj_k = tf.boolean_mask(labels, labels_obj_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(train_init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, bbox, label, label_1, label_2, mask1 = sess.run([image_augment, bbox_result, label_result, labels_obj, labels_noobj, labels_obj_mask])\n",
    "#image, label = sess.run([image_augment, label_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bbox = bbox[0]\n",
    "for i in range(image_bbox.shape[0]):\n",
    "    cv2.rectangle(image[0], (image_bbox[i][0],image_bbox[i][1]), (image_bbox[i][2],image_bbox[i][3]), (0,255,0), 2)\n",
    "plt.imshow(image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encpasulate the conv and residual functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv(inputs, filters, kernel_size, strides, padding, bias=False, normalize=True, activation='relu'):\n",
    "    output = inputs\n",
    "    padding_str = 'same'\n",
    "    if padding>0:\n",
    "        output = l.ZeroPadding2D(padding=padding)(output)\n",
    "        padding_str = 'valid'\n",
    "    output = l.Conv2D(filters, kernel_size, strides, padding_str, use_bias=bias, \\\n",
    "                 kernel_initializer='he_normal', \\\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(l=5e-4))(output)\n",
    "    if normalize:\n",
    "        output = l.BatchNormalization(axis=3)(output)\n",
    "    if activation=='relu':\n",
    "        output = l.ReLU()(output)\n",
    "    if activation=='relu6':\n",
    "        output = l.ReLU(max_value=6)(output)\n",
    "    if activation=='leaky_relu':\n",
    "        output = l.LeakyReLU(alpha=0.1)(output)\n",
    "    if activation=='sigmoid':\n",
    "        output = tf.keras.activations.sigmoid(output)\n",
    "    return output\n",
    "\n",
    "def _residual(inputs, filters, strides):\n",
    "    shortcut = inputs\n",
    "    num_channels = shortcut.get_shape().as_list()[-1]\n",
    "    output = _conv(inputs, filters, 3, strides, 1)\n",
    "    output = _conv(output, filters, 3, 1, 0, False, True, 'linear')\n",
    "    if num_channels != filters or strides != 1:\n",
    "        shortcut = _conv(shortcut, filters, 1, strides, 0, False, True, 'linear')\n",
    "    output = l.Add()([output, shortcut])\n",
    "    output = l.ReLU()(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hourglass function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hourglass(inputs, filterDim):\n",
    "    #Left part, left_features image dimension, 1,1/2,1/4,1/8,1/16,1/32\n",
    "    left_features = [inputs]\n",
    "    for index, dim in enumerate(filterDim):\n",
    "        output = _residual(left_features[-1], dim, 2)\n",
    "        output = _residual(output, dim, 1)\n",
    "        left_features.append(output)\n",
    "    #Middle part\n",
    "    output = left_features[-1]\n",
    "    for i in range(5):\n",
    "        output = _residual(output, filterDim[-1], 1)\n",
    "    #Right part\n",
    "    for index in reversed(range(len(filterDim))):\n",
    "        output = _residual(output, filterDim[index], 1)\n",
    "        output = _residual(output, filterDim[max(index-1, 0)], 1)\n",
    "        output = l.UpSampling2D()(output)\n",
    "        left_feature = _residual(left_features[index], filterDim[max(index-1, 0)], 1)\n",
    "        left_feature = _residual(left_feature, filterDim[max(index-1, 0)], 1)\n",
    "        output = l.Add()([output, left_feature])\n",
    "    output = _conv(output, 256, 3, 1, 1)\n",
    "    outputs = [output]\n",
    "    #Prediction\n",
    "    pred_keypoints = _conv(output, 256, 3, 1, 0, True, False, 'relu') \n",
    "    pred_keypoints = _conv(pred_keypoints, 80, 1, 1, 0, True, False, 'sigmoid') \n",
    "    outputs.append(pred_keypoints)\n",
    "    pred_offset = _conv(output, 256, 3, 1, 0, True, False, 'relu') \n",
    "    pred_offset = _conv(pred_offset, 2, 1, 1, 0, True, False, 'linear') \n",
    "    outputs.append(pred_offset)\n",
    "    pred_size = _conv(output, 256, 3, 1, 0, True, False, 'relu') \n",
    "    pred_size = _conv(pred_size, 2, 1, 1, 0, True, False, 'linear') \n",
    "    outputs.append(pred_size)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the base hourglass network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassNetwork():\n",
    "    image = tf.keras.Input(shape=(imageHeight,imageWidth,3))   #512×512×3\n",
    "    net = _conv(image, 128, 7, 2, 3)    #256×256×128\n",
    "    net = _residual(net, 256, 2)        #128×128×256\n",
    "    outputs_1 = _hourglass(net, filterDim)\n",
    "    outputs_2 = _hourglass(outputs_1[0], filterDim)\n",
    "    model = tf.keras.Model(inputs=image, outputs=(outputs_1+outputs_2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds, labels):\n",
    "    labels = tf.reshape(labels, [-1, imageHeight//4, imageWidth//4, 85])\n",
    "    labels_offset_size = labels[...,80:]\n",
    "    labels_offset_size_mask = tf.equal(labels_offset_size[...,0],1.0)\n",
    "    labels_offset_size_obj = tf.boolean_mask(labels_offset_size, labels_offset_size_mask)\n",
    "    #obj_num = tf.cast(labels_offset_size_obj.shape[0], tf.float32)\n",
    "    obj_num = tf.cast(tf.shape(labels_offset_size_obj)[0], tf.float32)\n",
    "    preds_offset = preds[1]\n",
    "    preds_offset_obj = tf.boolean_mask(preds_offset, labels_offset_size_mask)\n",
    "    loss_offset = tf.math.truediv(\\\n",
    "                      tf.reduce_sum(\\\n",
    "                          tf.math.abs(\\\n",
    "                              preds_offset_obj-labels_offset_size_obj[...,1:3])),\n",
    "                      obj_num)\n",
    "    preds_size = preds[2]\n",
    "    preds_size_obj = tf.boolean_mask(preds_size, labels_offset_size_mask)\n",
    "    loss_size = tf.math.truediv(\\\n",
    "                      tf.reduce_sum(\\\n",
    "                          tf.math.abs(\\\n",
    "                              preds_size_obj-labels_offset_size_obj[...,3:])),\n",
    "                      obj_num)\n",
    "    labels_k = labels[...,:80]\n",
    "    labels_k_obj_mask = tf.equal(labels_k, 1.0)\n",
    "    labels_k_noobj_mask = tf.less(labels_k, 1.0)\n",
    "    labels_k_obj = tf.boolean_mask(labels_k, labels_k_obj_mask)\n",
    "    labels_k_noobj = tf.boolean_mask(labels_k, labels_k_noobj_mask)\n",
    "    keypoint = tf.nn.sigmoid(preds[0])\n",
    "    preds_k_obj = tf.boolean_mask(keypoint, labels_k_obj_mask)\n",
    "    preds_k_noobj = tf.boolean_mask(keypoint, labels_k_noobj_mask)\n",
    "    loss_k_obj = tf.reduce_sum(\\\n",
    "                     tf.math.multiply(\\\n",
    "                         tf.math.pow((1.0-preds_k_obj), focal_alpha),\\\n",
    "                         tf.math.log(preds_k_obj+1e-5)))\n",
    "    loss_k_noobj = tf.reduce_sum(\\\n",
    "                       tf.math.multiply(\\\n",
    "                           tf.math.multiply(\\\n",
    "                               tf.math.pow((1.0-labels_k_noobj), focal_beta),\n",
    "                               tf.math.pow(preds_k_noobj, focal_alpha)),\n",
    "                           tf.math.log(1.0-preds_k_noobj+1e-5)))\n",
    "    loss_k = tf.truediv(\\\n",
    "                 tf.math.negative(loss_k_obj+loss_k_noobj),\\\n",
    "                 obj_num) \n",
    "    loss = loss_k + lambda_size*loss_size + lambda_offset*loss_offset\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate the output to prediction bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(keypoint, offset, size):\n",
    "    keypoint_nms = l.MaxPool2D(3, 1, 'same')(keypoint)\n",
    "    keypoint_mask = tf.cast(tf.equal(keypoint, keypoint_nms), tf.float32)\n",
    "    keypoint = keypoint * keypoint_mask\n",
    "    keypoint_shape = tf.shape(keypoint)\n",
    "    batch = keypoint_shape[0]\n",
    "    height = keypoint_shape[1]\n",
    "    width = keypoint_shape[2]\n",
    "    category = keypoint_shape[3]\n",
    "    keypoint_flat = tf.reshape(keypoint, [batch, -1])\n",
    "    offset_flat = tf.reshape(offset, [batch, -1, 2])\n",
    "    size_flat = tf.reshape(size, [batch, -1, 2])\n",
    "    scores, indices = tf.math.top_k(keypoint_flat, k=10, sorted=True)\n",
    "    classes = tf.cast(indices%category, tf.float32)\n",
    "    indices = tf.cast(indices/category, tf.int32)\n",
    "    x = tf.cast(indices%width, tf.float32)\n",
    "    y = tf.cast(indices/height, tf.float32)\n",
    "    offset_x = tf.batch_gather(offset_flat[...,1], indices)\n",
    "    offset_y = tf.batch_gather(offset_flat[...,0], indices)\n",
    "    size_w = tf.batch_gather(size_flat[...,1], indices)\n",
    "    size_h = tf.batch_gather(size_flat[...,0], indices)\n",
    "    x = x + offset_x\n",
    "    y = y + offset_y\n",
    "    xmin = (x - size_w/2)*4\n",
    "    xmax = (x + size_w/2)*4\n",
    "    ymin = (y - size_h/2)*4\n",
    "    ymax = (y + size_h/2)*4\n",
    "    bbox = tf.stack([xmin,xmax,ymin,ymax,scores,classes], axis=-1)\n",
    "    return bbox\n",
    "\n",
    "keypoint = tf.random.uniform([3,8,8,4])\n",
    "offset = tf.random.uniform([3,8,8,2])\n",
    "size = tf.random.uniform([3,8,8,2],minval=0,maxval=100)\n",
    "bbox = predict(keypoint,offset,size)\n",
    "bbox_mask = tf.greater(bbox[...,4],0.99)\n",
    "bbox_nms = tf.boolean_mask(bbox, bbox_mask)\n",
    "\n",
    "sess=tf.Session()\n",
    "k,o,s,b,b_nms = sess.run([keypoint,offset,size, bbox, bbox_nms])\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.random.uniform([3,6,6,2])\n",
    "b=tf.greater(a[...,0], 0.5)\n",
    "c=tf.boolean_mask(a,b)\n",
    "sess=tf.Session()\n",
    "d,e,f=sess.run([a,b,c])\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(keypoint, offset, size):\n",
    "    keypoint_nms = l.MaxPool2D(3, 1, 'same')(keypoint)\n",
    "    keypoint_mask = tf.cast(tf.equal(keypoint, keypoint_nms), tf.float32)\n",
    "    keypoint = keypoint * keypoint_mask\n",
    "    keypoint_shape = tf.shape(keypoint)\n",
    "    batch = keypoint_shape[0]\n",
    "    height = keypoint_shape[1]\n",
    "    width = keypoint_shape[2]\n",
    "    category = keypoint_shape[3]\n",
    "    keypoint_flat = tf.reshape(keypoint, [batch, -1])\n",
    "    offset_flat = tf.reshape(offset, [batch, -1, 2])\n",
    "    size_flat = tf.reshape(size, [batch, -1, 2])\n",
    "    scores, indices = tf.math.top_k(keypoint_flat, k=10, sorted=True)\n",
    "    classes = tf.cast(indices%category, tf.float32)\n",
    "    indices = tf.cast(indices/category, tf.int32)\n",
    "    x = tf.cast(indices%width, tf.float32)\n",
    "    y = tf.cast(indices/height, tf.float32)\n",
    "    offset_x = tf.batch_gather(offset_flat[...,1], indices)\n",
    "    offset_y = tf.batch_gather(offset_flat[...,0], indices)\n",
    "    size_w = tf.batch_gather(size_flat[...,1], indices)\n",
    "    size_h = tf.batch_gather(size_flat[...,0], indices)\n",
    "    x = x + offset_x\n",
    "    y = y + offset_y\n",
    "    xmin = (x - size_w/2)*4\n",
    "    xmax = (x + size_w/2)*4\n",
    "    ymin = (y - size_h/2)*4\n",
    "    ymax = (y + size_h/2)*4\n",
    "    bbox = tf.stack([xmin,xmax,ymin,ymax,scores,classes], axis=-1)\n",
    "    return bbox\n",
    "                                \n",
    "def CenterModel(features, labels, mode, params):\n",
    "    model = HourglassNetwork()\n",
    "    training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    images = tf.reshape(features[\"images\"], [-1, imageHeight, imageWidth, 3])\n",
    "    #images = tf.reshape(features[0], [-1, imageHeight, imageWidth, 3])\n",
    "    outputs = model(images, training)\n",
    "    print('bbb')\n",
    "    print(outputs)\n",
    "    print(labels)\n",
    " \n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"bboxes\": predict(outputs[5], outputs[6], outputs[7])\n",
    "    }\n",
    " \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions, \\\n",
    "                                          export_outputs={'classify': tf.estimator.export.PredictOutput(predictions)})\n",
    " \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss_inter = loss_fn(outputs[1:4], labels)\n",
    "    loss_final = loss_fn(outputs[5:], labels)\n",
    "    loss = loss_inter + loss_final\n",
    " \n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        '''\n",
    "        boundaries = [5000, 60000, 80000]\n",
    "        values = [0.1, 0.01, 0.001, 0.0001]\n",
    "        learning_rate = tf.compat.v1.train.piecewise_constant(global_step, boundaries, values)\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "        '''\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(model.get_updates_for(features)):\n",
    "            train_op = optimizer.minimize(loss=loss, global_step=global_step)\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    ''' \n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    m = tf.keras.metrics.sparse_top_k_categorical_accuracy(y_true=labels,  y_pred=logits)\n",
    "    tf.summary.scalar('top-5_accuracy', m)\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "    tf.summary.scalar('accuracy', accuracy[0]) \n",
    "    eval_metric_ops = {\n",
    "        #\"accuracy\": tf.metrics.accuracy(labels=true_labels, predictions=predictions[\"classes\"])}\n",
    "        \"accuracy\": accuracy} \n",
    "        #\"top-5 accuracy\": (m.result(), m.update_state(y_true=labels, y_pred=logits))}\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 14:32:48.736530 139942699554560 estimator.py:1790] Using default config.\n",
      "I0924 14:32:48.738181 139942699554560 estimator.py:209] Using config: {'_model_dir': '/home/roy/AI/centermodel/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f46685f7da0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "W0924 14:32:48.761960 139942699554560 deprecation.py:323] From /home/roy/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0924 14:32:48.919597 139942699554560 deprecation.py:323] From /home/roy/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0924 14:32:48.993573 139942699554560 deprecation.py:323] From <ipython-input-2-d4f2875263ae>:101: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "I0924 14:32:49.023575 139942699554560 estimator.py:1145] Calling model_fn.\n",
      "W0924 14:32:59.711338 139942699554560 deprecation.py:323] From /home/roy/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims` instead.\n",
      "W0924 14:32:59.818361 139942699554560 deprecation.py:323] From /home/roy/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbb\n",
      "[<tf.Tensor 'model/re_lu_73/Relu:0' shape=(?, 128, 128, 256) dtype=float32>, <tf.Tensor 'model/tf_op_layer_Sigmoid/Sigmoid:0' shape=(?, 128, 128, 80) dtype=float32>, <tf.Tensor 'model/conv2d_85/BiasAdd:0' shape=(?, 128, 128, 2) dtype=float32>, <tf.Tensor 'model/conv2d_87/BiasAdd:0' shape=(?, 128, 128, 2) dtype=float32>, <tf.Tensor 'model/re_lu_147/Relu:0' shape=(?, 128, 128, 256) dtype=float32>, <tf.Tensor 'model/tf_op_layer_Sigmoid_1/Sigmoid_1:0' shape=(?, 128, 128, 80) dtype=float32>, <tf.Tensor 'model/conv2d_169/BiasAdd:0' shape=(?, 128, 128, 2) dtype=float32>, <tf.Tensor 'model/conv2d_171/BiasAdd:0' shape=(?, 128, 128, 2) dtype=float32>]\n",
      "Tensor(\"IteratorGetNext:1\", dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 14:33:10.364601 139942699554560 estimator.py:1147] Done calling model_fn.\n",
      "I0924 14:33:10.367045 139942699554560 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0924 14:33:19.488030 139942699554560 monitored_session.py:240] Graph was finalized.\n",
      "W0924 14:33:19.793993 139942699554560 deprecation.py:323] From /home/roy/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "I0924 14:33:19.795884 139942699554560 saver.py:1280] Restoring parameters from /home/roy/AI/centermodel/model.ckpt-0\n",
      "W0924 14:33:27.702707 139942699554560 deprecation.py:323] From /home/roy/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "I0924 14:33:30.277201 139942699554560 session_manager.py:500] Running local_init_op.\n",
      "I0924 14:33:31.300434 139942699554560 session_manager.py:502] Done running local_init_op.\n",
      "I0924 14:34:01.398783 139942699554560 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /home/roy/AI/centermodel/model.ckpt.\n",
      "I0924 14:34:44.514823 139942699554560 basic_session_run_hooks.py:262] loss = 168463.72, step = 0\n",
      "/home/roy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/roy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  from ipykernel import kernelapp as app\n",
      "I0924 14:35:50.794733 139942699554560 basic_session_run_hooks.py:692] global_step/sec: 1.50873\n",
      "I0924 14:35:50.797032 139942699554560 basic_session_run_hooks.py:260] loss = 59034.31, step = 100 (66.282 sec)\n",
      "I0924 14:36:31.453410 139942699554560 basic_session_run_hooks.py:692] global_step/sec: 2.4595\n",
      "I0924 14:36:31.455671 139942699554560 basic_session_run_hooks.py:260] loss = 56726.508, step = 200 (40.659 sec)\n",
      "I0924 14:37:12.393737 139942699554560 basic_session_run_hooks.py:692] global_step/sec: 2.44258\n",
      "I0924 14:37:12.395609 139942699554560 basic_session_run_hooks.py:260] loss = 22298.498, step = 300 (40.940 sec)\n",
      "I0924 14:37:53.499828 139942699554560 basic_session_run_hooks.py:692] global_step/sec: 2.43273\n",
      "I0924 14:37:53.501847 139942699554560 basic_session_run_hooks.py:260] loss = 170210.06, step = 400 (41.106 sec)\n",
      "I0924 14:38:34.734700 139942699554560 basic_session_run_hooks.py:692] global_step/sec: 2.42513\n",
      "I0924 14:38:34.736230 139942699554560 basic_session_run_hooks.py:260] loss = 52370.367, step = 500 (41.234 sec)\n",
      "I0924 14:39:16.005450 139942699554560 basic_session_run_hooks.py:692] global_step/sec: 2.42302\n",
      "I0924 14:39:16.007377 139942699554560 basic_session_run_hooks.py:260] loss = 50435.79, step = 600 (41.271 sec)\n",
      "I0924 14:39:57.580008 139942699554560 basic_session_run_hooks.py:692] global_step/sec: 2.40532\n",
      "I0924 14:39:57.582267 139942699554560 basic_session_run_hooks.py:260] loss = 52395.45, step = 700 (41.575 sec)\n",
      "I0924 14:40:39.388180 139942699554560 basic_session_run_hooks.py:692] global_step/sec: 2.39188\n",
      "I0924 14:40:39.389569 139942699554560 basic_session_run_hooks.py:260] loss = 104733.99, step = 800 (41.807 sec)\n",
      "I0924 14:41:21.196719 139942699554560 basic_session_run_hooks.py:692] global_step/sec: 2.39185\n",
      "I0924 14:41:21.197853 139942699554560 basic_session_run_hooks.py:260] loss = 97114.64, step = 900 (41.808 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4f603e88af61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4f603e88af61>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                  params={'feature_columns': my_feature_columns,})\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mimagenet_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#eval_results = imagenet_classifier.evaluate(input_fn=val_input_fn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(eval_results)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1190\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1191\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1482\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many_step_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1250\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m             run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1253\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m         logging.info(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1336\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(_):\n",
    "    my_feature_columns = []\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key='images', shape=(imageHeight,imageWidth, 3)))\n",
    "    imagenet_classifier = tf.estimator.Estimator(model_fn=CenterModel, \\\n",
    "                                                 model_dir=\"/home/roy/AI/centermodel/\", \\\n",
    "                                                 params={'feature_columns': my_feature_columns,})\n",
    "    for _ in range(10):\n",
    "        imagenet_classifier.train(input_fn=train_input_fn, steps=5000)\n",
    "        #eval_results = imagenet_classifier.evaluate(input_fn=val_input_fn)\n",
    "        #print(eval_results)\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.math.log(0.001)\n",
    "sess = tf.Session()\n",
    "b=sess.run(a)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
